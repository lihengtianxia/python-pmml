<?xml version='1.0' encoding='UTF-8'?>
<Header copyright="Copyright (c) 2018 NIST" description="DenseNet121">
  <DataDictionary numberOfFields="1">
    <DataField channels="3" dataType="image" height="300" name="I" width="300"/>
  </DataDictionary>
  <DeepNeuralNetwork modelname="Deep Neural Network" functionname="regression">
    <Layer name="input_4" type="InputLayer">
      <InputSize>
        <Array n="3" type="int">224 224 3</Array>
      </InputSize>
    </Layer>
    <Layer name="zero_padding2d_1" type="ZeroPadding2D">
      <Padding>
        <Array n="4" type="int">3 3 3 3</Array>
      </Padding>
    </Layer>
    <Layer activation="linear" inbound_node="zero_padding2d_1" name="conv1/conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="64">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">7 7</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">2 2</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv1/bn"/>
    <Layer type="Activation" activation="relu" name="conv1/relu"/>
    <Layer name="zero_padding2d_2" type="ZeroPadding2D">
      <Padding>
        <Array n="4" type="int">1 1 1 1</Array>
      </Padding>
    </Layer>
    <Layer name="pool1" type="MaxPooling2D">
      <PoolSize>
        <Array n="2" type="int">3 3</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block1_0_relu" name="conv2_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block1_1_relu" name="conv2_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool1 conv2_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block2_0_relu" name="conv2_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block2_1_relu" name="conv2_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block1_concat conv2_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block3_0_relu" name="conv2_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block3_1_relu" name="conv2_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block2_concat conv2_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block4_0_relu" name="conv2_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block4_1_relu" name="conv2_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block3_concat conv2_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block5_0_relu" name="conv2_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block5_1_relu" name="conv2_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block4_concat conv2_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block6_0_relu" name="conv2_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block6_1_relu" name="conv2_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block5_concat conv2_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool2_bn"/>
    <Layer type="Activation" activation="relu" name="pool2_relu"/>
    <Layer activation="linear" inbound_node="pool2_relu" name="pool2_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool2_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block1_0_relu" name="conv3_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block1_1_relu" name="conv3_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool2_pool conv3_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block2_0_relu" name="conv3_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block2_1_relu" name="conv3_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block1_concat conv3_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block3_0_relu" name="conv3_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block3_1_relu" name="conv3_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block2_concat conv3_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block4_0_relu" name="conv3_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block4_1_relu" name="conv3_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block3_concat conv3_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block5_0_relu" name="conv3_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block5_1_relu" name="conv3_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block4_concat conv3_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block6_0_relu" name="conv3_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block6_1_relu" name="conv3_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block5_concat conv3_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block7_0_relu" name="conv3_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block7_1_relu" name="conv3_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block6_concat conv3_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block8_0_relu" name="conv3_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block8_1_relu" name="conv3_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block7_concat conv3_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block9_0_relu" name="conv3_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block9_1_relu" name="conv3_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block8_concat conv3_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block10_0_relu" name="conv3_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block10_1_relu" name="conv3_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block9_concat conv3_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block11_0_relu" name="conv3_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block11_1_relu" name="conv3_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block10_concat conv3_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block12_0_relu" name="conv3_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block12_1_relu" name="conv3_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block11_concat conv3_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool3_bn"/>
    <Layer type="Activation" activation="relu" name="pool3_relu"/>
    <Layer activation="linear" inbound_node="pool3_relu" name="pool3_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="256">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool3_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block1_0_relu" name="conv4_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block1_1_relu" name="conv4_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool3_pool conv4_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block2_0_relu" name="conv4_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block2_1_relu" name="conv4_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block1_concat conv4_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block3_0_relu" name="conv4_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block3_1_relu" name="conv4_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block2_concat conv4_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block4_0_relu" name="conv4_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block4_1_relu" name="conv4_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block3_concat conv4_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block5_0_relu" name="conv4_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block5_1_relu" name="conv4_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block4_concat conv4_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block6_0_relu" name="conv4_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block6_1_relu" name="conv4_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block5_concat conv4_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block7_0_relu" name="conv4_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block7_1_relu" name="conv4_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block6_concat conv4_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block8_0_relu" name="conv4_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block8_1_relu" name="conv4_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block7_concat conv4_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block9_0_relu" name="conv4_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block9_1_relu" name="conv4_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block8_concat conv4_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block10_0_relu" name="conv4_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block10_1_relu" name="conv4_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block9_concat conv4_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block11_0_relu" name="conv4_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block11_1_relu" name="conv4_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block10_concat conv4_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block12_0_relu" name="conv4_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block12_1_relu" name="conv4_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block11_concat conv4_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block13_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block13_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block13_0_relu" name="conv4_block13_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block13_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block13_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block13_1_relu" name="conv4_block13_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block13_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block12_concat conv4_block13_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block14_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block14_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block14_0_relu" name="conv4_block14_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block14_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block14_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block14_1_relu" name="conv4_block14_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block14_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block13_concat conv4_block14_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block15_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block15_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block15_0_relu" name="conv4_block15_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block15_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block15_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block15_1_relu" name="conv4_block15_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block15_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block14_concat conv4_block15_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block16_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block16_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block16_0_relu" name="conv4_block16_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block16_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block16_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block16_1_relu" name="conv4_block16_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block16_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block15_concat conv4_block16_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block17_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block17_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block17_0_relu" name="conv4_block17_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block17_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block17_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block17_1_relu" name="conv4_block17_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block17_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block16_concat conv4_block17_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block18_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block18_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block18_0_relu" name="conv4_block18_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block18_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block18_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block18_1_relu" name="conv4_block18_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block18_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block17_concat conv4_block18_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block19_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block19_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block19_0_relu" name="conv4_block19_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block19_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block19_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block19_1_relu" name="conv4_block19_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block19_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block18_concat conv4_block19_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block20_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block20_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block20_0_relu" name="conv4_block20_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block20_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block20_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block20_1_relu" name="conv4_block20_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block20_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block19_concat conv4_block20_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block21_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block21_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block21_0_relu" name="conv4_block21_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block21_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block21_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block21_1_relu" name="conv4_block21_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block21_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block20_concat conv4_block21_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block22_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block22_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block22_0_relu" name="conv4_block22_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block22_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block22_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block22_1_relu" name="conv4_block22_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block22_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block21_concat conv4_block22_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block23_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block23_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block23_0_relu" name="conv4_block23_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block23_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block23_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block23_1_relu" name="conv4_block23_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block23_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block22_concat conv4_block23_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block24_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block24_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block24_0_relu" name="conv4_block24_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block24_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block24_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block24_1_relu" name="conv4_block24_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block24_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block23_concat conv4_block24_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool4_bn"/>
    <Layer type="Activation" activation="relu" name="pool4_relu"/>
    <Layer activation="linear" inbound_node="pool4_relu" name="pool4_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="512">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool4_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block1_0_relu" name="conv5_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block1_1_relu" name="conv5_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool4_pool conv5_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block2_0_relu" name="conv5_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block2_1_relu" name="conv5_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block1_concat conv5_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block3_0_relu" name="conv5_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block3_1_relu" name="conv5_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block2_concat conv5_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block4_0_relu" name="conv5_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block4_1_relu" name="conv5_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block3_concat conv5_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block5_0_relu" name="conv5_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block5_1_relu" name="conv5_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block4_concat conv5_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block6_0_relu" name="conv5_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block6_1_relu" name="conv5_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block5_concat conv5_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block7_0_relu" name="conv5_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block7_1_relu" name="conv5_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block6_concat conv5_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block8_0_relu" name="conv5_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block8_1_relu" name="conv5_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block7_concat conv5_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block9_0_relu" name="conv5_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block9_1_relu" name="conv5_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block8_concat conv5_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block10_0_relu" name="conv5_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block10_1_relu" name="conv5_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block9_concat conv5_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block11_0_relu" name="conv5_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block11_1_relu" name="conv5_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block10_concat conv5_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block12_0_relu" name="conv5_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block12_1_relu" name="conv5_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block11_concat conv5_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block13_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block13_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block13_0_relu" name="conv5_block13_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block13_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block13_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block13_1_relu" name="conv5_block13_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block13_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block12_concat conv5_block13_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block14_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block14_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block14_0_relu" name="conv5_block14_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block14_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block14_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block14_1_relu" name="conv5_block14_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block14_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block13_concat conv5_block14_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block15_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block15_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block15_0_relu" name="conv5_block15_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block15_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block15_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block15_1_relu" name="conv5_block15_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block15_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block14_concat conv5_block15_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block16_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block16_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block16_0_relu" name="conv5_block16_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block16_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block16_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block16_1_relu" name="conv5_block16_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block16_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block15_concat conv5_block16_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="bn"/>
    <Layer type="Activation" activation="relu" name="relu"/>
    <Layer name="avg_pool" type="GlobalAveragePooling2D"/>
    <Layer activation="softmax" channels="1000" name="fc1000" type="Dense"/>
    <Weights encoding="hdf5" href="weights.h5"/>
  </DeepNeuralNetwork>
</Header>
