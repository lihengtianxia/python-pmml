<?xml version='1.0' encoding='UTF-8'?>
<Header copyright="Copyright (c) 2018 NIST" description="DenseNet201">
  <DataDictionary numberOfFields="1">
    <DataField channels="3" dataType="image" height="300" name="I" width="300"/>
  </DataDictionary>
  <DeepNeuralNetwork modelname="Deep Neural Network" functionname="regression">
    <Layer name="input_4" type="InputLayer">
      <InputSize>
        <Array n="3" type="int">224 224 3</Array>
      </InputSize>
    </Layer>
    <Layer name="zero_padding2d_1" type="ZeroPadding2D">
      <Padding>
        <Array n="4" type="int">3 3 3 3</Array>
      </Padding>
    </Layer>
    <Layer activation="linear" inbound_node="zero_padding2d_1" name="conv1/conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="64">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">7 7</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">2 2</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv1/bn"/>
    <Layer type="Activation" activation="relu" name="conv1/relu"/>
    <Layer name="zero_padding2d_2" type="ZeroPadding2D">
      <Padding>
        <Array n="4" type="int">1 1 1 1</Array>
      </Padding>
    </Layer>
    <Layer name="pool1" type="MaxPooling2D">
      <PoolSize>
        <Array n="2" type="int">3 3</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block1_0_relu" name="conv2_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block1_1_relu" name="conv2_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool1 conv2_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block2_0_relu" name="conv2_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block2_1_relu" name="conv2_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block1_concat conv2_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block3_0_relu" name="conv2_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block3_1_relu" name="conv2_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block2_concat conv2_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block4_0_relu" name="conv2_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block4_1_relu" name="conv2_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block3_concat conv2_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block5_0_relu" name="conv2_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block5_1_relu" name="conv2_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block4_concat conv2_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv2_block6_0_relu" name="conv2_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv2_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv2_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv2_block6_1_relu" name="conv2_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv2_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv2_block5_concat conv2_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool2_bn"/>
    <Layer type="Activation" activation="relu" name="pool2_relu"/>
    <Layer activation="linear" inbound_node="pool2_relu" name="pool2_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool2_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block1_0_relu" name="conv3_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block1_1_relu" name="conv3_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool2_pool conv3_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block2_0_relu" name="conv3_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block2_1_relu" name="conv3_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block1_concat conv3_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block3_0_relu" name="conv3_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block3_1_relu" name="conv3_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block2_concat conv3_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block4_0_relu" name="conv3_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block4_1_relu" name="conv3_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block3_concat conv3_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block5_0_relu" name="conv3_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block5_1_relu" name="conv3_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block4_concat conv3_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block6_0_relu" name="conv3_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block6_1_relu" name="conv3_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block5_concat conv3_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block7_0_relu" name="conv3_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block7_1_relu" name="conv3_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block6_concat conv3_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block8_0_relu" name="conv3_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block8_1_relu" name="conv3_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block7_concat conv3_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block9_0_relu" name="conv3_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block9_1_relu" name="conv3_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block8_concat conv3_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block10_0_relu" name="conv3_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block10_1_relu" name="conv3_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block9_concat conv3_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block11_0_relu" name="conv3_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block11_1_relu" name="conv3_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block10_concat conv3_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv3_block12_0_relu" name="conv3_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv3_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv3_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv3_block12_1_relu" name="conv3_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv3_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv3_block11_concat conv3_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool3_bn"/>
    <Layer type="Activation" activation="relu" name="pool3_relu"/>
    <Layer activation="linear" inbound_node="pool3_relu" name="pool3_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="256">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool3_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block1_0_relu" name="conv4_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block1_1_relu" name="conv4_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool3_pool conv4_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block2_0_relu" name="conv4_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block2_1_relu" name="conv4_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block1_concat conv4_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block3_0_relu" name="conv4_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block3_1_relu" name="conv4_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block2_concat conv4_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block4_0_relu" name="conv4_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block4_1_relu" name="conv4_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block3_concat conv4_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block5_0_relu" name="conv4_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block5_1_relu" name="conv4_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block4_concat conv4_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block6_0_relu" name="conv4_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block6_1_relu" name="conv4_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block5_concat conv4_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block7_0_relu" name="conv4_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block7_1_relu" name="conv4_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block6_concat conv4_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block8_0_relu" name="conv4_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block8_1_relu" name="conv4_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block7_concat conv4_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block9_0_relu" name="conv4_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block9_1_relu" name="conv4_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block8_concat conv4_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block10_0_relu" name="conv4_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block10_1_relu" name="conv4_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block9_concat conv4_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block11_0_relu" name="conv4_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block11_1_relu" name="conv4_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block10_concat conv4_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block12_0_relu" name="conv4_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block12_1_relu" name="conv4_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block11_concat conv4_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block13_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block13_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block13_0_relu" name="conv4_block13_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block13_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block13_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block13_1_relu" name="conv4_block13_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block13_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block12_concat conv4_block13_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block14_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block14_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block14_0_relu" name="conv4_block14_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block14_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block14_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block14_1_relu" name="conv4_block14_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block14_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block13_concat conv4_block14_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block15_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block15_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block15_0_relu" name="conv4_block15_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block15_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block15_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block15_1_relu" name="conv4_block15_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block15_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block14_concat conv4_block15_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block16_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block16_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block16_0_relu" name="conv4_block16_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block16_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block16_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block16_1_relu" name="conv4_block16_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block16_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block15_concat conv4_block16_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block17_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block17_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block17_0_relu" name="conv4_block17_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block17_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block17_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block17_1_relu" name="conv4_block17_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block17_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block16_concat conv4_block17_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block18_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block18_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block18_0_relu" name="conv4_block18_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block18_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block18_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block18_1_relu" name="conv4_block18_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block18_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block17_concat conv4_block18_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block19_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block19_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block19_0_relu" name="conv4_block19_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block19_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block19_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block19_1_relu" name="conv4_block19_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block19_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block18_concat conv4_block19_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block20_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block20_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block20_0_relu" name="conv4_block20_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block20_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block20_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block20_1_relu" name="conv4_block20_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block20_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block19_concat conv4_block20_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block21_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block21_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block21_0_relu" name="conv4_block21_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block21_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block21_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block21_1_relu" name="conv4_block21_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block21_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block20_concat conv4_block21_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block22_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block22_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block22_0_relu" name="conv4_block22_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block22_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block22_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block22_1_relu" name="conv4_block22_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block22_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block21_concat conv4_block22_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block23_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block23_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block23_0_relu" name="conv4_block23_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block23_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block23_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block23_1_relu" name="conv4_block23_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block23_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block22_concat conv4_block23_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block24_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block24_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block24_0_relu" name="conv4_block24_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block24_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block24_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block24_1_relu" name="conv4_block24_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block24_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block23_concat conv4_block24_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block25_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block25_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block25_0_relu" name="conv4_block25_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block25_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block25_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block25_1_relu" name="conv4_block25_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block25_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block24_concat conv4_block25_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block26_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block26_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block26_0_relu" name="conv4_block26_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block26_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block26_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block26_1_relu" name="conv4_block26_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block26_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block25_concat conv4_block26_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block27_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block27_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block27_0_relu" name="conv4_block27_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block27_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block27_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block27_1_relu" name="conv4_block27_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block27_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block26_concat conv4_block27_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block28_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block28_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block28_0_relu" name="conv4_block28_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block28_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block28_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block28_1_relu" name="conv4_block28_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block28_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block27_concat conv4_block28_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block29_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block29_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block29_0_relu" name="conv4_block29_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block29_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block29_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block29_1_relu" name="conv4_block29_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block29_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block28_concat conv4_block29_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block30_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block30_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block30_0_relu" name="conv4_block30_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block30_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block30_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block30_1_relu" name="conv4_block30_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block30_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block29_concat conv4_block30_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block31_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block31_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block31_0_relu" name="conv4_block31_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block31_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block31_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block31_1_relu" name="conv4_block31_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block31_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block30_concat conv4_block31_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block32_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block32_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block32_0_relu" name="conv4_block32_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block32_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block32_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block32_1_relu" name="conv4_block32_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block32_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block31_concat conv4_block32_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block33_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block33_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block33_0_relu" name="conv4_block33_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block33_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block33_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block33_1_relu" name="conv4_block33_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block33_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block32_concat conv4_block33_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block34_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block34_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block34_0_relu" name="conv4_block34_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block34_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block34_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block34_1_relu" name="conv4_block34_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block34_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block33_concat conv4_block34_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block35_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block35_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block35_0_relu" name="conv4_block35_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block35_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block35_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block35_1_relu" name="conv4_block35_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block35_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block34_concat conv4_block35_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block36_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block36_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block36_0_relu" name="conv4_block36_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block36_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block36_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block36_1_relu" name="conv4_block36_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block36_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block35_concat conv4_block36_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block37_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block37_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block37_0_relu" name="conv4_block37_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block37_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block37_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block37_1_relu" name="conv4_block37_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block37_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block36_concat conv4_block37_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block38_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block38_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block38_0_relu" name="conv4_block38_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block38_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block38_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block38_1_relu" name="conv4_block38_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block38_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block37_concat conv4_block38_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block39_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block39_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block39_0_relu" name="conv4_block39_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block39_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block39_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block39_1_relu" name="conv4_block39_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block39_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block38_concat conv4_block39_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block40_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block40_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block40_0_relu" name="conv4_block40_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block40_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block40_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block40_1_relu" name="conv4_block40_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block40_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block39_concat conv4_block40_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block41_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block41_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block41_0_relu" name="conv4_block41_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block41_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block41_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block41_1_relu" name="conv4_block41_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block41_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block40_concat conv4_block41_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block42_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block42_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block42_0_relu" name="conv4_block42_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block42_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block42_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block42_1_relu" name="conv4_block42_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block42_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block41_concat conv4_block42_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block43_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block43_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block43_0_relu" name="conv4_block43_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block43_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block43_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block43_1_relu" name="conv4_block43_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block43_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block42_concat conv4_block43_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block44_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block44_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block44_0_relu" name="conv4_block44_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block44_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block44_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block44_1_relu" name="conv4_block44_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block44_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block43_concat conv4_block44_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block45_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block45_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block45_0_relu" name="conv4_block45_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block45_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block45_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block45_1_relu" name="conv4_block45_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block45_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block44_concat conv4_block45_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block46_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block46_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block46_0_relu" name="conv4_block46_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block46_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block46_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block46_1_relu" name="conv4_block46_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block46_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block45_concat conv4_block46_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block47_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block47_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block47_0_relu" name="conv4_block47_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block47_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block47_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block47_1_relu" name="conv4_block47_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block47_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block46_concat conv4_block47_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block48_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block48_0_relu"/>
    <Layer activation="linear" inbound_node="conv4_block48_0_relu" name="conv4_block48_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv4_block48_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv4_block48_1_relu"/>
    <Layer activation="linear" inbound_node="conv4_block48_1_relu" name="conv4_block48_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv4_block48_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv4_block47_concat conv4_block48_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="pool4_bn"/>
    <Layer type="Activation" activation="relu" name="pool4_relu"/>
    <Layer activation="linear" inbound_node="pool4_relu" name="pool4_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="896">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer name="pool4_pool" type="AveragePooling2D">
      <PoolSize>
        <Array n="2" type="int">2 2</Array>
      </PoolSize>
      <Strides>
        <Array n="2" type="int">2 2</Array>
      </Strides>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block1_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block1_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block1_0_relu" name="conv5_block1_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block1_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block1_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block1_1_relu" name="conv5_block1_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block1_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">pool4_pool conv5_block1_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block2_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block2_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block2_0_relu" name="conv5_block2_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block2_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block2_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block2_1_relu" name="conv5_block2_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block2_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block1_concat conv5_block2_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block3_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block3_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block3_0_relu" name="conv5_block3_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block3_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block3_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block3_1_relu" name="conv5_block3_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block3_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block2_concat conv5_block3_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block4_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block4_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block4_0_relu" name="conv5_block4_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block4_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block4_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block4_1_relu" name="conv5_block4_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block4_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block3_concat conv5_block4_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block5_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block5_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block5_0_relu" name="conv5_block5_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block5_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block5_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block5_1_relu" name="conv5_block5_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block5_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block4_concat conv5_block5_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block6_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block6_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block6_0_relu" name="conv5_block6_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block6_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block6_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block6_1_relu" name="conv5_block6_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block6_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block5_concat conv5_block6_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block7_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block7_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block7_0_relu" name="conv5_block7_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block7_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block7_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block7_1_relu" name="conv5_block7_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block7_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block6_concat conv5_block7_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block8_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block8_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block8_0_relu" name="conv5_block8_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block8_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block8_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block8_1_relu" name="conv5_block8_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block8_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block7_concat conv5_block8_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block9_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block9_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block9_0_relu" name="conv5_block9_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block9_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block9_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block9_1_relu" name="conv5_block9_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block9_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block8_concat conv5_block9_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block10_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block10_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block10_0_relu" name="conv5_block10_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block10_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block10_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block10_1_relu" name="conv5_block10_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block10_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block9_concat conv5_block10_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block11_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block11_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block11_0_relu" name="conv5_block11_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block11_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block11_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block11_1_relu" name="conv5_block11_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block11_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block10_concat conv5_block11_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block12_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block12_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block12_0_relu" name="conv5_block12_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block12_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block12_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block12_1_relu" name="conv5_block12_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block12_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block11_concat conv5_block12_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block13_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block13_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block13_0_relu" name="conv5_block13_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block13_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block13_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block13_1_relu" name="conv5_block13_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block13_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block12_concat conv5_block13_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block14_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block14_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block14_0_relu" name="conv5_block14_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block14_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block14_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block14_1_relu" name="conv5_block14_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block14_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block13_concat conv5_block14_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block15_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block15_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block15_0_relu" name="conv5_block15_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block15_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block15_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block15_1_relu" name="conv5_block15_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block15_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block14_concat conv5_block15_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block16_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block16_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block16_0_relu" name="conv5_block16_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block16_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block16_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block16_1_relu" name="conv5_block16_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block16_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block15_concat conv5_block16_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block17_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block17_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block17_0_relu" name="conv5_block17_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block17_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block17_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block17_1_relu" name="conv5_block17_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block17_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block16_concat conv5_block17_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block18_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block18_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block18_0_relu" name="conv5_block18_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block18_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block18_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block18_1_relu" name="conv5_block18_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block18_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block17_concat conv5_block18_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block19_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block19_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block19_0_relu" name="conv5_block19_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block19_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block19_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block19_1_relu" name="conv5_block19_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block19_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block18_concat conv5_block19_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block20_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block20_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block20_0_relu" name="conv5_block20_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block20_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block20_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block20_1_relu" name="conv5_block20_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block20_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block19_concat conv5_block20_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block21_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block21_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block21_0_relu" name="conv5_block21_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block21_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block21_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block21_1_relu" name="conv5_block21_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block21_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block20_concat conv5_block21_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block22_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block22_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block22_0_relu" name="conv5_block22_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block22_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block22_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block22_1_relu" name="conv5_block22_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block22_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block21_concat conv5_block22_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block23_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block23_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block23_0_relu" name="conv5_block23_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block23_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block23_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block23_1_relu" name="conv5_block23_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block23_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block22_concat conv5_block23_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block24_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block24_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block24_0_relu" name="conv5_block24_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block24_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block24_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block24_1_relu" name="conv5_block24_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block24_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block23_concat conv5_block24_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block25_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block25_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block25_0_relu" name="conv5_block25_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block25_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block25_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block25_1_relu" name="conv5_block25_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block25_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block24_concat conv5_block25_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block26_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block26_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block26_0_relu" name="conv5_block26_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block26_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block26_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block26_1_relu" name="conv5_block26_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block26_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block25_concat conv5_block26_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block27_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block27_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block27_0_relu" name="conv5_block27_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block27_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block27_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block27_1_relu" name="conv5_block27_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block27_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block26_concat conv5_block27_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block28_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block28_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block28_0_relu" name="conv5_block28_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block28_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block28_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block28_1_relu" name="conv5_block28_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block28_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block27_concat conv5_block28_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block29_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block29_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block29_0_relu" name="conv5_block29_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block29_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block29_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block29_1_relu" name="conv5_block29_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block29_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block28_concat conv5_block29_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block30_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block30_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block30_0_relu" name="conv5_block30_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block30_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block30_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block30_1_relu" name="conv5_block30_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block30_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block29_concat conv5_block30_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block31_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block31_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block31_0_relu" name="conv5_block31_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block31_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block31_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block31_1_relu" name="conv5_block31_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block31_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block30_concat conv5_block31_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block32_0_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block32_0_relu"/>
    <Layer activation="linear" inbound_node="conv5_block32_0_relu" name="conv5_block32_1_conv" padding="valid" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="128">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">1 1</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="conv5_block32_1_bn"/>
    <Layer type="Activation" activation="relu" name="conv5_block32_1_relu"/>
    <Layer activation="linear" inbound_node="conv5_block32_1_relu" name="conv5_block32_2_conv" padding="same" type="Conv2D" use_bias="False">
      <ConvolutionalKernel channels="32">
        <DilationRate>
          <Array n="2" type="int">1 1</Array>
        </DilationRate>
        <KernelSize>
          <Array n="2" type="int">3 3</Array>
        </KernelSize>
        <KernelStride>
          <Array n="2" type="int">1 1</Array>
        </KernelStride>
      </ConvolutionalKernel>
    </Layer>
    <Layer type="Merge" name="conv5_block32_concat" operator="concatenate">
      <Inputs>
        <Array n="2" type="string">conv5_block31_concat conv5_block32_2_conv</Array>
      </Inputs>
    </Layer>
    <Layer type="BatchNormalization" axis="3" center="True" momentum="0.99" name="bn"/>
    <Layer type="Activation" activation="relu" name="relu"/>
    <Layer name="avg_pool" type="GlobalAveragePooling2D"/>
    <Layer activation="softmax" channels="1000" name="fc1000" type="Dense"/>
    <Weights encoding="hdf5" href="weights.h5"/>
  </DeepNeuralNetwork>
</Header>
